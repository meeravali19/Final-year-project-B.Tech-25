{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "onqidg1crkMK",
    "outputId": "621520c0-c2e0-4a7b-f5bd-e9ada5d86fae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [03:49:47] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ XGBoost Accuracy: 83.37%\n",
      "✅ Classification Report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "       BENIGN       1.00      1.00      1.00     60869\n",
      "    DrDoS_DNS       0.76      0.27      0.40     59959\n",
      "   DrDoS_LDAP       0.50      0.66      0.57     59953\n",
      "  DrDoS_MSSQL       0.92      0.95      0.93     59978\n",
      "    DrDoS_NTP       0.99      1.00      0.99     59297\n",
      "DrDoS_NetBIOS       0.95      0.98      0.97     59978\n",
      "   DrDoS_SNMP       0.66      0.84      0.74     59985\n",
      "   DrDoS_SSDP       0.99      0.96      0.97     59981\n",
      "\n",
      "     accuracy                           0.83    480000\n",
      "    macro avg       0.85      0.83      0.82    480000\n",
      " weighted avg       0.85      0.83      0.82    480000\n",
      "\n",
      "\n",
      "📁 XGBoost model saved as 'ddos_xgboost_model.json'\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import xgboost as xgb\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"Final_dataset+20000.csv\")  # Replace with actual path\n",
    "\n",
    "# Data preprocessing\n",
    "X = df.drop(columns=['Label'])  # Features\n",
    "y = df['Label']  # Target\n",
    "\n",
    "# Encode target labels (for multiclass classification)\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "# Optional: Handle class imbalance (XGBoost handles imbalance well, so this step is optional)\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y), y=y)\n",
    "class_weights_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Initialize XGBoost classifier\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    objective='multi:softmax',          # Multiclass classification\n",
    "    num_class=len(np.unique(y)),        # Number of classes\n",
    "    eval_metric='mlogloss',\n",
    "    use_label_encoder=False,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Train the XGBoost model\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\n✅ XGBoost Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(\"✅ Classification Report:\\n\", classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
    "\n",
    "# Save the model\n",
    "xgb_model.save_model(\"ddos_xgboost_model.json\")\n",
    "print(\"\\n📁 XGBoost model saved as 'ddos_xgboost_model.json'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mcIofLTru2w6",
    "outputId": "90177d2c-f37b-46e4-ccfd-0113f6b77dff"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\meera\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m41500/41500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m514s\u001b[0m 12ms/step - accuracy: 0.7066 - loss: 0.7304 - val_accuracy: 0.7647 - val_loss: 0.5313\n",
      "Epoch 2/30\n",
      "\u001b[1m41500/41500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m566s\u001b[0m 14ms/step - accuracy: 0.7651 - loss: 0.5438 - val_accuracy: 0.7742 - val_loss: 0.5113\n",
      "Epoch 3/30\n",
      "\u001b[1m41500/41500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m915s\u001b[0m 22ms/step - accuracy: 0.7704 - loss: 0.5275 - val_accuracy: 0.7798 - val_loss: 0.5049\n",
      "Epoch 4/30\n",
      "\u001b[1m41500/41500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1915s\u001b[0m 46ms/step - accuracy: 0.7760 - loss: 0.5185 - val_accuracy: 0.7846 - val_loss: 0.4971\n",
      "Epoch 5/30\n",
      "\u001b[1m41500/41500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1329s\u001b[0m 32ms/step - accuracy: 0.7805 - loss: 0.5121 - val_accuracy: 0.7800 - val_loss: 0.5001\n",
      "Epoch 6/30\n",
      "\u001b[1m41500/41500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m523s\u001b[0m 13ms/step - accuracy: 0.7828 - loss: 0.5063 - val_accuracy: 0.7932 - val_loss: 0.4931\n",
      "Epoch 7/30\n",
      "\u001b[1m41500/41500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m563s\u001b[0m 14ms/step - accuracy: 0.7832 - loss: 0.5033 - val_accuracy: 0.7934 - val_loss: 0.4881\n",
      "Epoch 8/30\n",
      "\u001b[1m41500/41500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m570s\u001b[0m 14ms/step - accuracy: 0.7847 - loss: 0.5021 - val_accuracy: 0.7875 - val_loss: 0.4920\n",
      "Epoch 9/30\n",
      "\u001b[1m41500/41500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m554s\u001b[0m 13ms/step - accuracy: 0.7850 - loss: 0.4986 - val_accuracy: 0.7985 - val_loss: 0.4807\n",
      "Epoch 10/30\n",
      "\u001b[1m41500/41500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m654s\u001b[0m 16ms/step - accuracy: 0.7873 - loss: 0.4956 - val_accuracy: 0.7999 - val_loss: 0.4780\n",
      "Epoch 11/30\n",
      "\u001b[1m41500/41500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m622s\u001b[0m 15ms/step - accuracy: 0.7873 - loss: 0.4940 - val_accuracy: 0.7992 - val_loss: 0.4791\n",
      "Epoch 12/30\n",
      "\u001b[1m41500/41500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m633s\u001b[0m 15ms/step - accuracy: 0.7888 - loss: 0.4932 - val_accuracy: 0.7898 - val_loss: 0.4872\n",
      "Epoch 13/30\n",
      "\u001b[1m41500/41500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m624s\u001b[0m 15ms/step - accuracy: 0.7877 - loss: 0.4938 - val_accuracy: 0.7991 - val_loss: 0.4816\n",
      "Epoch 14/30\n",
      "\u001b[1m 9897/41500\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6:51\u001b[0m 13ms/step - accuracy: 0.7910 - loss: 0.4879"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(r\"Final_dataset+20000.csv\")\n",
    "\n",
    "# Features and labels\n",
    "X = df.drop(columns=['Label'])\n",
    "y = df['Label']\n",
    "\n",
    "# Label encoding and one-hot conversion\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "y_categorical = to_categorical(y_encoded)\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_encoded), y=y_encoded)\n",
    "class_weights_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_categorical, test_size=0.2, stratify=y_encoded, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Reshape for LSTM [samples, timesteps, features]\n",
    "X_train_lstm = X_train_scaled.reshape((X_train_scaled.shape[0], X_train_scaled.shape[1], 1))\n",
    "X_test_lstm = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))\n",
    "\n",
    "# Build LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(64, return_sequences=True, input_shape=(X_train_lstm.shape[1], 1)),\n",
    "    Dropout(0.3),\n",
    "    LSTM(32),\n",
    "    Dropout(0.3),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(y_categorical.shape[1], activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "model.fit(X_train_lstm, y_train, validation_data=(X_test_lstm, y_test), epochs=30, batch_size=32, class_weight=class_weights_dict)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_probs = model.predict(X_test_lstm)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_true, y_pred, target_names=label_encoder.classes_))\n",
    "\n",
    "# Save model\n",
    "model.save(\"ddos_lstm_multiclass_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
